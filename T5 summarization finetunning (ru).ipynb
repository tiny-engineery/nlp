{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T18:52:29.527583Z","iopub.execute_input":"2024-06-08T18:52:29.528163Z","iopub.status.idle":"2024-06-08T18:52:29.901116Z","shell.execute_reply.started":"2024-06-08T18:52:29.528128Z","shell.execute_reply":"2024-06-08T18:52:29.900329Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import BitsAndBytesConfig\n\nmodel_path = \"google/mt5-small\"\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_path,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:29.908181Z","iopub.execute_input":"2024-06-08T18:52:29.908457Z","iopub.status.idle":"2024-06-08T18:52:39.952815Z","shell.execute_reply.started":"2024-06-08T18:52:29.908433Z","shell.execute_reply":"2024-06-08T18:52:39.952024Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"IlyaGusev/gazeta\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:39.956214Z","iopub.execute_input":"2024-06-08T18:52:39.956737Z","iopub.status.idle":"2024-06-08T18:52:41.374937Z","shell.execute_reply.started":"2024-06-08T18:52:39.956711Z","shell.execute_reply":"2024-06-08T18:52:41.374179Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for IlyaGusev/gazeta contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/gazeta\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_subset = dataset['train'].select(range(6000))\ntest_subset = dataset['test'].select(range(600))\nvalidation_subset = dataset['validation'].select(range(600))\n\n# Combine them back into a DatasetDict if needed\nsubdataset = DatasetDict({\n    'train': train_subset,\n    'test': test_subset,\n    'validation': validation_subset\n})","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:41.385426Z","iopub.execute_input":"2024-06-08T18:52:41.386231Z","iopub.status.idle":"2024-06-08T18:52:41.399036Z","shell.execute_reply.started":"2024-06-08T18:52:41.386184Z","shell.execute_reply":"2024-06-08T18:52:41.398257Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def tokenize_fn(example):\n    model_inputs = tokenizer(example['text'], padding=True, truncation=True, max_length=512)\n    model_inputs[\"labels\"] = tokenizer(example['summary'], padding=True, truncation=True, max_length=512)['input_ids']\n    return model_inputs\n\ninputs = subdataset.map(tokenize_fn, batched = True)\ninputs = inputs.map(remove_columns=['title', 'date', 'url'])\n\ninputs.set_format(type='torch')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:41.400354Z","iopub.execute_input":"2024-06-08T18:52:41.400612Z","iopub.status.idle":"2024-06-08T18:52:41.524715Z","shell.execute_reply.started":"2024-06-08T18:52:41.400590Z","shell.execute_reply":"2024-06-08T18:52:41.523934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\nfrom transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:41.532853Z","iopub.execute_input":"2024-06-08T18:52:41.533123Z","iopub.status.idle":"2024-06-08T18:52:44.876763Z","shell.execute_reply.started":"2024-06-08T18:52:41.533100Z","shell.execute_reply":"2024-06-08T18:52:44.875899Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-06-08 18:52:42.383514: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 18:52:42.383568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 18:52:42.384939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:44.878029Z","iopub.execute_input":"2024-06-08T18:52:44.878846Z","iopub.status.idle":"2024-06-08T18:52:44.883604Z","shell.execute_reply.started":"2024-06-08T18:52:44.878814Z","shell.execute_reply":"2024-06-08T18:52:44.882498Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"test-trainer\", \n    num_train_epochs = 5,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_strategy=\"steps\",\n    logging_steps=100, \n    per_device_train_batch_size = 10,\n    per_device_eval_batch_size = 10,\n    report_to = 'none',\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:44.884836Z","iopub.execute_input":"2024-06-08T18:52:44.885193Z","iopub.status.idle":"2024-06-08T18:52:44.971452Z","shell.execute_reply.started":"2024-06-08T18:52:44.885165Z","shell.execute_reply":"2024-06-08T18:52:44.970661Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    training_args,\n    train_dataset=inputs[\"train\"],\n    eval_dataset=inputs[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:44.983870Z","iopub.execute_input":"2024-06-08T18:52:44.984321Z","iopub.status.idle":"2024-06-08T18:52:45.439900Z","shell.execute_reply.started":"2024-06-08T18:52:44.984282Z","shell.execute_reply":"2024-06-08T18:52:45.438881Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T18:52:45.441038Z","iopub.execute_input":"2024-06-08T18:52:45.441323Z","iopub.status.idle":"2024-06-08T19:29:17.461926Z","shell.execute_reply.started":"2024-06-08T18:52:45.441298Z","shell.execute_reply":"2024-06-08T19:29:17.460289Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3001' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 36:06, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>28.619500</td>\n      <td>13.089636</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>16.193800</td>\n      <td>8.211783</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>11.332000</td>\n      <td>6.855174</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>8.504800</td>\n      <td>6.262938</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>6.375500</td>\n      <td>5.398847</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.911500</td>\n      <td>4.550810</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.953600</td>\n      <td>3.081573</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.464100</td>\n      <td>2.537444</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.080800</td>\n      <td>2.275347</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.941200</td>\n      <td>2.003653</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.712400</td>\n      <td>1.909186</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.588300</td>\n      <td>1.846591</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.486000</td>\n      <td>1.810227</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.441000</td>\n      <td>1.764038</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.296200</td>\n      <td>1.738693</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.322100</td>\n      <td>1.723138</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.262900</td>\n      <td>1.707828</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.203000</td>\n      <td>1.689940</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.162500</td>\n      <td>1.681222</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.167500</td>\n      <td>1.672799</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.102500</td>\n      <td>1.666882</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.141600</td>\n      <td>1.656815</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.084000</td>\n      <td>1.653723</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.079700</td>\n      <td>1.648627</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.060500</td>\n      <td>1.647419</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.077000</td>\n      <td>1.647297</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.048100</td>\n      <td>1.643604</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.070400</td>\n      <td>1.643559</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.044100</td>\n      <td>1.643664</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.029400</td>\n      <td>1.643255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/562: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# batched_input = data_collator(inputs['train'])\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2815\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 2815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   2920\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   2921\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   2922\u001b[0m     )\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2924\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2925\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   2929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2930\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:424] . unexpected pos 1350317568 vs 1350317460"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:424] . unexpected pos 1350317568 vs 1350317460","output_type":"error"}]},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:31:47.930914Z","iopub.execute_input":"2024-06-08T19:31:47.931662Z","iopub.status.idle":"2024-06-08T19:31:48.004532Z","shell.execute_reply.started":"2024-06-08T19:31:47.931626Z","shell.execute_reply":"2024-06-08T19:31:48.003554Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"summarization\", \n                model=model, \n                tokenizer=tokenizer, \n                max_length = 40, \n               )","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:36:57.568718Z","iopub.execute_input":"2024-06-08T19:36:57.569110Z","iopub.status.idle":"2024-06-08T19:36:57.573923Z","shell.execute_reply.started":"2024-06-08T19:36:57.569076Z","shell.execute_reply":"2024-06-08T19:36:57.573014Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"my_inputs = \"Один раз школьник Ваня отправился в поход в горы с классом. Они выехали рано утром, и дорога заняла несколько часов. Когда автобус остановился, перед ними открылась величественная картина горных вершин. Ребята сразу же начали подниматься по тропинке, наслаждаясь свежим воздухом и красивыми видами. Ваня был в восторге от природы вокруг: цветы, деревья и маленькие ручейки создавали сказочную атмосферОни шли долго, иногда останавливаясь на привалы, чтобы отдохнуть и перекусить. Учитель рассказывал интересные факты о горах, и Ваня узнал много нового. В какой-то момент они добрались до вершины, откуда открывался потрясающий вид на долину. Ваня чувствовал себя на вершине мира и радовался, что смог покорить эту высоту. На обратном пути все делились впечатлениями и мечтали о новых походах. Этот день навсегда остался в памяти Вани как один из самых ярких и увлекательных.\"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:38:59.953957Z","iopub.execute_input":"2024-06-08T19:38:59.954347Z","iopub.status.idle":"2024-06-08T19:38:59.962214Z","shell.execute_reply.started":"2024-06-08T19:38:59.954317Z","shell.execute_reply":"2024-06-08T19:38:59.961225Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pipe(my_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:39:00.541657Z","iopub.execute_input":"2024-06-08T19:39:00.541945Z","iopub.status.idle":"2024-06-08T19:39:00.829156Z","shell.execute_reply.started":"2024-06-08T19:39:00.541921Z","shell.execute_reply":"2024-06-08T19:39:00.828297Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'В ходе школьника Ваня отправился в поход в горы с классом.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Итог\n\nНа Данный момент работает не слишком хорошо, однако обучение длилось всего 30 минут, поэтому результат приемлимый.","metadata":{}}]}