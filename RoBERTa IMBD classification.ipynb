{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T18:27:25.877779Z","iopub.execute_input":"2024-04-24T18:27:25.878621Z","iopub.status.idle":"2024-04-24T18:27:26.732737Z","shell.execute_reply.started":"2024-04-24T18:27:25.878586Z","shell.execute_reply":"2024-04-24T18:27:26.731993Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import RobertaTokenizerFast, RobertaModel, RobertaForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:01.579805Z","iopub.execute_input":"2024-04-24T18:28:01.581049Z","iopub.status.idle":"2024-04-24T18:28:08.371078Z","shell.execute_reply.started":"2024-04-24T18:28:01.581012Z","shell.execute_reply":"2024-04-24T18:28:08.370291Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# model = RobertaModel.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base')\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)\n\n# tokenizer(\" Hello world\")['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:08.372563Z","iopub.execute_input":"2024-04-24T18:28:08.373068Z","iopub.status.idle":"2024-04-24T18:28:12.621835Z","shell.execute_reply.started":"2024-04-24T18:28:08.373042Z","shell.execute_reply":"2024-04-24T18:28:12.620662Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db429b51e1e94e34844d110014eb7475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22afc5d1d8304b8fae4869155b447782"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbffaaa9fefa402cab2ab242ffa92b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d4c17e4f7d41cb806c5b6ad6babc31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656d886a99ef45ae90c17ede5d000efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dba426004b64def93f0bfc8c198411e"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:12.623329Z","iopub.execute_input":"2024-04-24T18:28:12.624052Z","iopub.status.idle":"2024-04-24T18:28:12.634990Z","shell.execute_reply.started":"2024-04-24T18:28:12.624009Z","shell.execute_reply":"2024-04-24T18:28:12.633998Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# dataset['train']['text'][25]\ndef tokenize_fn(text):\n    tokenized =  tokenizer(text['text'], \n                   truncation = True, \n                   max_length = 512, \n                   return_tensors='pt',\n                  )\n    \n    return tokenized\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:12.637376Z","iopub.execute_input":"2024-04-24T18:28:12.637763Z","iopub.status.idle":"2024-04-24T18:28:12.742922Z","shell.execute_reply.started":"2024-04-24T18:28:12.637730Z","shell.execute_reply":"2024-04-24T18:28:12.741826Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import datasets\ntrain_data_imbd, test_data_imbd = datasets.load_dataset('imdb', split =['train', 'test'])\ntrain_data_imbd = train_data_imbd.shuffle(seed=42)\ntest_data_imbd = test_data_imbd.shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:12.744134Z","iopub.execute_input":"2024-04-24T18:28:12.744415Z","iopub.status.idle":"2024-04-24T18:28:23.398965Z","shell.execute_reply.started":"2024-04-24T18:28:12.744386Z","shell.execute_reply":"2024-04-24T18:28:23.397991Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ae9b87a78e45faa3585e67dc9109d1"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 44.6MB/s]\nDownloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 86.3MB/s]\nDownloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 126MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adac4d52661b4a2c84b5dc4bdd0736ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6914563681064c84a684c293ee2f34fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3546006d2d064cba9631ea0ff5bd0aaf"}},"metadata":{}}]},{"cell_type":"code","source":"# train_data_imbd['label'][3]","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:28:23.400211Z","iopub.execute_input":"2024-04-24T18:28:23.400768Z","iopub.status.idle":"2024-04-24T18:28:23.405201Z","shell.execute_reply.started":"2024-04-24T18:28:23.400734Z","shell.execute_reply":"2024-04-24T18:28:23.404254Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_len = 2000\ntrain_dataset_imbd = train_data_imbd.select(list(range(train_len)))\ntest_dataset_imbd = test_data_imbd.select(list(range(train_len//10)))\n# train_dataset_imbd = train_data_imbd\n# test_dataset_imbd = test_data_imbd\ntrain_dataset_imbd\n# train_data_2['label'][200]","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:29:30.602377Z","iopub.execute_input":"2024-04-24T18:29:30.603103Z","iopub.status.idle":"2024-04-24T18:29:30.625433Z","shell.execute_reply.started":"2024-04-24T18:29:30.603067Z","shell.execute_reply":"2024-04-24T18:29:30.624476Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Finetune","metadata":{}},{"cell_type":"code","source":"# train_data_2['text'][2]\ndef tokenize_fn(batched_text):\n    output = tokenizer(batched_text['text'], padding=True, truncation=True, return_tensors='pt')\n    return output\n\ndef rename_column(example):\n    example['deleteme'] = example['label']\n    del example['label']\n    return example\n\ntrain_imbd = train_dataset_imbd.map(tokenize_fn, batched = True)\ntest_imbd = test_dataset_imbd.map(tokenize_fn, batched = True)\n\ntrain_imbd = train_imbd.map(rename_column)\ntest_imbd = test_imbd.map(rename_column)\n\ntrain_imbd.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_imbd.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n# train_imbd.set_format(type='torch', columns=['input_ids', 'attention_mask', 'deleteme'])\n# test_imbd.set_format(type='torch', columns=['input_ids', 'attention_mask', 'deleteme'])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:27.080708Z","iopub.execute_input":"2024-04-24T18:30:27.081101Z","iopub.status.idle":"2024-04-24T18:30:27.166216Z","shell.execute_reply.started":"2024-04-24T18:30:27.081072Z","shell.execute_reply":"2024-04-24T18:30:27.165355Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# type(train_imbd['input_ids'][3])\ntrain_imbd","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:29.183440Z","iopub.execute_input":"2024-04-24T18:30:29.183790Z","iopub.status.idle":"2024-04-24T18:30:29.189583Z","shell.execute_reply.started":"2024-04-24T18:30:29.183764Z","shell.execute_reply":"2024-04-24T18:30:29.188651Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'deleteme'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"code","source":"# define accuracy metrics\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\nfrom datasets import load_metric\n\nmetric = load_metric('accuracy')\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:30.902535Z","iopub.execute_input":"2024-04-24T18:30:30.902919Z","iopub.status.idle":"2024-04-24T18:30:31.167565Z","shell.execute_reply.started":"2024-04-24T18:30:30.902888Z","shell.execute_reply":"2024-04-24T18:30:31.166683Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir = '/kaggle/working/results',\n    num_train_epochs=3,    \n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    logging_strategy=\"steps\",\n    logging_steps=25,\n    logging_dir='/kaggle/working/logs',\n    report_to = 'none',\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:34.182437Z","iopub.execute_input":"2024-04-24T18:30:34.183298Z","iopub.status.idle":"2024-04-24T18:30:34.208587Z","shell.execute_reply.started":"2024-04-24T18:30:34.183263Z","shell.execute_reply":"2024-04-24T18:30:34.207814Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# instantiate the trainer class and check for available devices\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_imbd,\n    eval_dataset=test_imbd, \n)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:35.061125Z","iopub.execute_input":"2024-04-24T18:30:35.061842Z","iopub.status.idle":"2024-04-24T18:30:35.084015Z","shell.execute_reply.started":"2024-04-24T18:30:35.061810Z","shell.execute_reply":"2024-04-24T18:30:35.083056Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:30:37.140278Z","iopub.execute_input":"2024-04-24T18:30:37.140940Z","iopub.status.idle":"2024-04-24T18:30:37.819464Z","shell.execute_reply.started":"2024-04-24T18:30:37.140908Z","shell.execute_reply":"2024-04-24T18:30:37.818207Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3077\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3077\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3078\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3079\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3080\u001b[0m         )\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3082\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."],"ename":"ValueError","evalue":"The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.","output_type":"error"}]}]}